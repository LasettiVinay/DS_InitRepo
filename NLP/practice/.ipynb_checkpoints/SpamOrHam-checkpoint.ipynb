{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\\SPAM text message 20170820 - Data.csv\n"
     ]
    }
   ],
   "source": [
    "# get test and train dataframes\n",
    "rawTextData = os.path.join('dataset', 'SPAM text message 20170820 - Data.csv')\n",
    "print(rawTextData)\n",
    "text_df = pd.read_csv(rawTextData, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data type \n",
    "type(text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      "Category    5572 non-null object\n",
      "Message     5572 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# use .info() to get the basic information about the data frame\n",
    "text_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5     spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6      ham  Even my brother is not like to speak with me. ...\n",
       "7      ham  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8     spam  WINNER!! As a valued network customer you have...\n",
       "9     spam  Had your mobile 11 months or more? U R entitle..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Please don't text me anymore. I have nothing else to say.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df['Message'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "Ok lar... Joking wif u oni...\n",
      "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "U dun say so early hor... U c already then say...\n",
      "Nah I don't think he goes to usf, he lives around here though\n"
     ]
    }
   ],
   "source": [
    "### Encoding ham to 0 and spam to 1\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for i, label in enumerate(text_df['Category']):\n",
    "    #print(text)\n",
    "    texts.append(text_df['Message'][i])\n",
    "    if label == 'ham':\n",
    "        labels.append(0)\n",
    "    else:\n",
    "        labels.append(1)\n",
    "\n",
    "print(labels[0:20])\n",
    "\n",
    "for i in range(5):\n",
    "    print(texts[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = np.asarray(texts)\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important to note: this is imbalanced data\n",
    "\n",
    "####### There are more ham than spam mails\n",
    "\n",
    "####### Ratio -- ham: spam = 6.5 : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[4825  747]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(labels))\n",
    "print(np.bincount(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.476510067114094"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4825 / 745"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle data and split 80% data for train and 20% for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# shuffle data\n",
    "indices = np.arange(text_df.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "texts = texts[indices]\n",
    "labels = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_percent = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Success, 80% training data, 20% test data\n",
      "train_X data size: 4457,   test_x data size: 1115\n",
      "train_Y data size: 4457,   test_y data size: 1115\n"
     ]
    }
   ],
   "source": [
    "# Split data 80%:20%\n",
    "training_samples = int(len(indices)*split_percent)\n",
    "\n",
    "train_data_X = texts[0:training_samples]\n",
    "train_data_Y = labels[0:training_samples]\n",
    "\n",
    "test_data_x = texts[training_samples:]\n",
    "test_data_y = labels[training_samples:]\n",
    "\n",
    "# Test split\n",
    "if len(texts) == len(train_data_X)+len(test_data_x):\n",
    "    print(\"Split Success, 80% training data, 20% test data\")\n",
    "    print(\"train_X data size: {},   test_x data size: {}\".format(len(train_data_X), len(test_data_x)))\n",
    "    print(\"train_Y data size: {},   test_y data size: {}\".format(len(train_data_Y), len(test_data_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of wods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<4457x7773 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 59237 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "BAG OF WORDS MODEL TO SPAM HAM train_data_X\n",
    "'''\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countVect = CountVectorizer().fit(train_data_X)\n",
    "X_train = countVect.transform(train_data_X)\n",
    "print(repr(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4457, 7773)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4457, 7773)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4457, 7773)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7773\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train.toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = countVect.transform(test_data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3864,  593], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(train_data_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ELASVIN\\AppData\\Local\\Continuum\\anaconda3\\envs\\sknlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "grid = GridSearchCV(logreg, param_grid, cv=5)\n",
    "logreg_train = grid.fit(X_train, train_data_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is:  0.9820627802690582\n"
     ]
    }
   ],
   "source": [
    "# logreg = LogisticRegression(C=0.1).fit(X_train, y_train)\n",
    "pred_logreg = logreg_train.predict(X_test)\n",
    "print(\"accuracy is: \", grid.score(X_test, test_data_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: this is imbalance datasets and a simple accuracy is not reliable (always predicting a major class will result in high accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thus important to check confusion matrix !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[958   3]\n",
      " [ 17 137]]\n"
     ]
    }
   ],
   "source": [
    "confusion = confusion_matrix(test_data_y, pred_logreg)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to imporve ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7773\n",
      "\n",
      "\n",
      "['00', '000', '008704050406', '0121', '01223585236', '01223585334', '0125698789', '02', '0207', '02072069400', '02073162414', '02085076972', '021', '03', '04', '0430', '05', '050703', '0578', '06']\n",
      "\n",
      "\n",
      "['yun', 'yunny', 'yuo', 'yuou', 'yup', 'yupz', 'zac', 'zaher', 'zealand', 'zebra', 'zed', 'zeros', 'zindgi', 'zoe', 'zogtorius', 'zoom', 'zouk', 'zyada', 'ú1', '〨ud']\n",
      "\n",
      "\n",
      "['00', '2morrow', 'ad', 'bags', 'bye', 'convey', 'dogwood', 'fatty', 'goodnite', 'icicibank', 'lacking', 'matrix3', 'nigro', 'phd', 'rates', 'search', 'spell', 'tenants', 'unknown', 'woman']\n"
     ]
    }
   ],
   "source": [
    "features_names = countVect.get_feature_names()\n",
    "print(len(features_names))\n",
    "print(\"\\n\")\n",
    "# print first 20 features\n",
    "print(features_names[:20])\n",
    "print(\"\\n\")\n",
    "# print last 20 features\n",
    "print(features_names[-20:])\n",
    "print(\"\\n\")\n",
    "# print every 50th word\n",
    "print(features_names[::400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As we saw above, there are many uninformative features(words) such as numbers\n",
    "## We will use only the words that appear in at least 3 emails(documents) -- In other words, we will use frequent words which are also likely to be in the test set¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<4457x2400 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 52624 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "# min_df controls this condition(min_df=3 means pick up words which appear\n",
    "# at least 3 documents)\n",
    "vect = CountVectorizer(min_df=3).fit(train_data_X)\n",
    "\n",
    "X_train = vect.transform(train_data_X)\n",
    "X_test = vect.transform(test_data_x)\n",
    "print(repr(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n",
      "\n",
      "\n",
      "['00', '000', '02', '03', '04', '05', '06', '07xxxxxxxxx', '0800', '08000839402', '08000930705', '08000938767', '08001950382', '08002986906', '0808', '0845', '08452810073', '0870', '08701417012', '08707509020']\n",
      "\n",
      "\n",
      "['yer', 'yes', 'yest', 'yesterday', 'yet', 'yetunde', 'yijue', 'ym', 'yo', 'yoga', 'yogasana', 'you', 'your', 'yours', 'yourself', 'yr', 'yrs', 'yummy', 'yup', 'zed']\n",
      "\n",
      "\n",
      "['00', 'budget', 'files', 'lets', 'pobox', 'sunny']\n"
     ]
    }
   ],
   "source": [
    "features_names = vect.get_feature_names()\n",
    "print(len(features_names))\n",
    "print(\"\\n\")\n",
    "# print first 20 features\n",
    "print(features_names[:20])\n",
    "print(\"\\n\")\n",
    "# print last 20 features\n",
    "print(features_names[-20:])\n",
    "print(\"\\n\")\n",
    "# print every 50th word\n",
    "print(features_names[::400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ELASVIN\\AppData\\Local\\Continuum\\anaconda3\\envs\\sknlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "grid = GridSearchCV(logreg, param_grid, cv=5)\n",
    "logreg_train = grid.fit(X_train, train_data_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is:  0.9802690582959641\n"
     ]
    }
   ],
   "source": [
    "# logreg = LogisticRegression(C=0.1).fit(X_train, y_train)\n",
    "pred_logreg = logreg_train.predict(X_test)\n",
    "print(\"accuracy is: \", grid.score(X_test, test_data_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False positive rate and false negative rate decreased by 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[956   5]\n",
      " [ 17 137]]\n"
     ]
    }
   ],
   "source": [
    "confusion = confusion_matrix(test_data_y, pred_logreg)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another way to imporve this accuracy is to remove stop words¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words is : 318 \n",
      "\n",
      "Examples:  ['we', 'ourselves', 'down', 'had', 'beyond', 'three', 'anyhow', 'wherever', 'then', 'whoever', 'side', 'for', 'con', 'although', 'off', 'that', 'could', 'ltd', 'go', 'a', 'rather', 'and', 'with', 'thereafter', 'how', 'too', 'anything', 'otherwise', 'whether', 'back', 'never', 'between']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "print(\"Number of stop words is :\", len(ENGLISH_STOP_WORDS), \"\\n\")\n",
    "print(\"Examples: \", list(ENGLISH_STOP_WORDS)[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<4457x2175 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 28170 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=3, stop_words='english').fit(train_data_X)\n",
    "X_train = vect.transform(train_data_X)\n",
    "X_test = vect.transform(test_data_x)\n",
    "print(repr(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now there are x (2404 - 2178 ) less features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ELASVIN\\AppData\\Local\\Continuum\\anaconda3\\envs\\sknlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "grid = GridSearchCV(logreg, param_grid, cv=5)\n",
    "logreg_train = grid.fit(X_train, train_data_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is:  0.9829596412556054\n",
      "confusion matrix \n",
      " [[960   1]\n",
      " [ 18 136]]\n"
     ]
    }
   ],
   "source": [
    "# logreg = LogisticRegression(C=0.1).fit(X_train, y_train)\n",
    "pred_logreg = logreg_train.predict(X_test)\n",
    "print(\"accuracy is: \", grid.score(X_test, test_data_y))\n",
    "confusion = confusion_matrix(test_data_y, pred_logreg)\n",
    "print(\"confusion matrix \\n\", confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Move down 4 - gird serach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(logreg, param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_train = grid.fit(X_train, train_data_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logreg_train = logreg.fit(X_train, train_data_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logreg = LogisticRegression(C=0.1).fit(X_train, y_train)\n",
    "pred_logreg = logreg_train.predict(X_test)\n",
    "print(\"accuracy is: \", grid.score(X_test, test_data_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(test_data_y, pred_logreg)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Move Down 3  -- BAG OF WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countVect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "toy_samples = [\"It is sunny today and I like it, \", \n",
    "               \"she does not like hamburger\"]\n",
    "\n",
    "# tokenize document in words\n",
    "countVect.fit(toy_samples)\n",
    "print(\"tokenization\")\n",
    "print(countVect.vocabulary_, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform document into a matrix(the number indicates the number of words showing up in the document)\n",
    "bag_of_words = countVect.transform(toy_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bag_of_words)\n",
    "print(countVect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transformed sparse matrix is: \")\n",
    "print(bag_of_words.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Extra practice on bag of words model\n",
    "'''\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countVect = CountVectorizer()\n",
    "\n",
    "toy_samples = ['This is the first document.',\n",
    "               'This document is the second document.',\n",
    "              'And this is the third one.',\n",
    "              'Is this the first document?',]\n",
    "\n",
    "\n",
    "# tokenize document in words\n",
    "countVect.fit(toy_samples)\n",
    "print(\"tokenization\")\n",
    "print(countVect.vocabulary_, \"\\n\")\n",
    "\n",
    "# transform document into a matrix(the number indicates the number of words showing up in the document)\n",
    "bag_of_words = countVect.transform(toy_samples)\n",
    "\n",
    "print(countVect.get_feature_names())\n",
    "\n",
    "print(\"Transformed sparse matrix is: \")\n",
    "print(bag_of_words.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Move down 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_data_x[0] in train_data_X:\n",
    "    print(\"Index: \", np.where(train_data_X == test_data_x[0]))\n",
    "    print(\"Found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a numpy array from a list of numbers\n",
    "arr = np.array([11, 12, 13, 14, 15, 16, 17, 15, 11, 12, 14, 15, 16, 17])\n",
    "\n",
    "\n",
    "# Get the index of elements with value 15\n",
    "result = np.where(arr == 15)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listVar = 'A B C D E F G H I J K L M N O P Q R S T U V W X Y Z'.split(' ')\n",
    "print(len(listVar))\n",
    "print(listVar.index('C'))\n",
    "print(listVar[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(indices))\n",
    "print(training_samples+2)\n",
    "print(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_X[-3:])\n",
    "print(test_data_x[0])\n",
    "print(train_data_Y[-5:])\n",
    "print(test_data_y[:5])\n",
    "\n",
    "print(np.unique(test_data_y))\n",
    "print(np.bincount(test_data_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texts[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texts[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move down 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5,min_df=2,stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(text_df['Message'])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 100, n_init = 1, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.unique(km.labels_, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text={}\n",
    "for i,cluster in enumerate(km.labels_):\n",
    "    oneDocument = text_df['Message'][i]\n",
    "    if cluster not in text.keys():\n",
    "        text[cluster] = oneDocument\n",
    "    else:\n",
    "        text[cluster] += oneDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_stopwords = set(stopwords.words('english') + list(punctuation)+[\"million\",\"billion\",\"year\",\"millions\",\"billions\",\"y/y\",\"'s\",\"''\",\"``\"])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = {}\n",
    "counts={}\n",
    "for cluster in range(3):\n",
    "    word_sent = word_tokenize(text[cluster].lower())\n",
    "    word_sent=[word for word in word_sent if word not in _stopwords]\n",
    "    freq = FreqDist(word_sent)\n",
    "    keywords[cluster] = nlargest(100, freq, key=freq.get)\n",
    "    counts[cluster]=freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_keys={}\n",
    "for cluster in range(3):   \n",
    "    other_clusters=list(set(range(3))-set([cluster]))\n",
    "    keys_other_clusters=set(keywords[other_clusters[0]]).union(set(keywords[other_clusters[1]]))\n",
    "    unique=set(keywords[cluster])-keys_other_clusters\n",
    "    unique_keys[cluster]=nlargest(10, unique, key=counts[cluster].get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"Facebook Inc. has been giving advertisers an inflated metric for the average time users spent watching a video, a measurement that may have helped boost marketer spending on one of Facebook’s most popular ad products. The company, owner of the world’s largest social network, only counts a video as viewed if it has been seen for more than 3 seconds. The metric it gave advertisers for their average video view time incorporated only the people who had watched the video long enough to count as a view in the first place, inflating the metric because it didn’t count anyone who didn’t watch, or watched for a shorter time. Facebook’s stock fell more than 1.5 percent in extended trading after the miscalculation was earlier reported in the Wall Street Journal. Facebook had disclosed the mistake in a posting on its advertiser help center web page several weeks ago. Big advertising buyers and marketers are upset about the inflated metric, and asked the company for more details, according to the report in the Journal, citing unidentified people familiar with the situation. The Menlo Park, California-based company has kept revenue surging in part because of enthusiasm for its video ads, which advertisers compare in performance to those on Twitter, YouTube and around the web.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "classifier.fit(X,km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=vectorizer.transform([article.decode('utf8').encode('ascii',errors='ignore')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests import session\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload\n",
    "payload = {\n",
    "    'action'   : 'login',\n",
    "    'username' : 'lasetti',\n",
    "    'password' : 'vinnu456'\n",
    "}\n",
    "\n",
    "url = 'https://www.kaggle.com/team-ai/spam-text-message-classification/downloads/SPAM%20text%20message%2020170820%20-%20Data.csv/1'\n",
    "\n",
    "with session() as c:\n",
    "    c.post('https://www.kaggle.com/account/login', data=payload)\n",
    "    response = c.get(url)\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Move down - Initial Data extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kaggle datasets list -s spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kaggle datasets download -d team-ai/spam-text-message-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -ltr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "#import zipfile\n",
    "#with zipfile.ZipFile(\"spam-text-message-classification.zip\",\"r\") as zip_ref:\n",
    "#    zip_ref.extractall(\"dataset\")\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -ltr dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sknlp]",
   "language": "python",
   "name": "conda-env-sknlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
